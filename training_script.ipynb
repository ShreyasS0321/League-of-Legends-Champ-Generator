{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5449ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e68b8a9be5a4eb7ad36841c91243a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/340 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"  # or your path\n",
    "\n",
    "# 1. Load the dataset (from JSONL)\n",
    "dataset = load_dataset(\"json\", data_files=\"data/finetune_champion_gen.jsonl\", split=\"train\")\n",
    "\n",
    "# 2. Load the model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# 3. Tokenize\n",
    "def tokenize(example):\n",
    "    prompt = example[\"prompt\"] + \"\\n\"\n",
    "    response = example[\"response\"]\n",
    "    full_prompt = prompt + response\n",
    "\n",
    "    tokens = tokenizer(\n",
    "        full_prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024,\n",
    "    )\n",
    "    labels = tokens[\"input_ids\"].copy()\n",
    "    prompt_len = len(tokenizer(prompt)[\"input_ids\"])  # includes bos if present\n",
    "\n",
    "    # Mask out the prompt tokens from labels\n",
    "    labels[:prompt_len] = [-100] * prompt_len\n",
    "    tokens[\"labels\"] = labels\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False)\n",
    "\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2513d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (0.46.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from bitsandbytes) (2.5.1+cu118)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.5)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "76b9f166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f409ca2f2994ec99f6a817b979345fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,             # This enables QLoRA\n",
    "    bnb_4bit_quant_type=\"nf4\",     # This is the quant type you want (for Llama)\n",
    "    bnb_4bit_use_double_quant=True,  # Optional, improves some quantization\n",
    "    bnb_4bit_compute_dtype=\"bfloat16\"  # Use \"bfloat16\" or \"float16\" for Ampere GPUs\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device_map,\n",
    "    quantization_config=bnb_config,   # Only this, no load_in_4bit\n",
    "    torch_dtype=\"auto\",              # optional, you can specify bfloat16/float16 if you want\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13863ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (2.5.1+cu118)\n",
      "Requirement already satisfied: transformers in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (4.50.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (1.3.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (0.5.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from peft) (0.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2024.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from huggingface_hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch>=1.13.0->peft) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.12.14)\n",
      "Using cached peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.15.2\n"
     ]
    }
   ],
   "source": [
    "!pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80d7f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,             # LoRA rank (memory/performance tradeoff, 8-16 is common)\n",
    "    lora_alpha=16,   # Scaling (double r is default)\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()  # sanity check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dada06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b935ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.70.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\shrey\\anaconda3\\envs\\huggingface_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bae9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9164c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [128000, 4110, 264, 9130, 315, 42986, 18824, 2728, 279, 2768, 3649, 512, 17046, 25, 5473, 12393, 1232, 364, 13655, 518, 364, 12393, 5796, 8438, 1232, 14226, 8011, 518, 364, 12393, 1311, 4469, 1232, 364, 18, 518, 364, 12393, 1311, 4469, 5796, 8438, 1232, 14226, 15, 13, 20, 518, 364, 43648, 1232, 364, 1987, 518, 364, 43648, 5796, 8438, 1232, 14226, 19, 13, 23, 518, 364, 21208, 41995, 1232, 364, 1399, 518, 364, 21208, 41995, 5796, 8438, 1232, 14226, 20, 518, 364, 38551, 5023, 380, 1232, 364, 843, 518, 364, 38551, 5023, 380, 5796, 8438, 1232, 14226, 17, 13, 2304, 518, 364, 3479, 17374, 1232, 364, 12901, 518, 364, 21208, 9897, 1232, 364, 10005, 518, 364, 21208, 17374, 5796, 8438, 1232, 14226, 17, 13, 20, 39303, 364, 9416, 1232, 364, 3561, 268, 518, 364, 9416, 1311, 4469, 1232, 364, 13655, 489, 8011, 518, 364, 9416, 1311, 4469, 5796, 8438, 1232, 76452, 35003, 5144, 25, 16290, 1347, 5248, 800, 685, 11, 578, 12538, 258, 37080, 11, 16290, 1347, 5248, 800, 685, 11, 24218, 42743, 37770, 11, 4435, 469, 910, 198, 1999, 25, 510, 9514, 649, 923, 35641, 596, 538, 1618, 422, 499, 1390, 933, 32215, 279, 2539, 18824, 3649, 320, 2150, 11, 18000, 449, 28887, 11, 323, 264, 52322, 14646, 8, 304, 279, 1742, 315, 65806, 596, 4033, 18824, 6959, 627, 1163, 13580, 25, 362, 36436, 87, 198, 3936, 25, 279, 12538, 258, 37080, 198, 17046, 512, 14884, 25, 220, 13655, 198, 14884, 824, 2237, 25, 489, 8011, 198, 14884, 1239, 268, 25, 220, 18, 198, 14884, 1239, 268, 824, 2237, 25, 489, 15, 13, 20, 198, 63982, 25, 220, 1987, 198, 63982, 824, 2237, 25, 489, 19, 13, 23, 198, 29702, 5674, 25, 220, 1399, 198, 29702, 5674, 824, 2237, 25, 489, 20, 198, 44638, 22884, 25, 220, 843, 198, 44638, 22884, 824, 2237, 25, 489, 17, 13, 2304, 198, 10061, 4732, 25, 220, 12901, 198, 29702, 2134, 25, 220, 10005, 198, 29702, 4732, 824, 2237, 25, 489, 17, 13, 20, 14062, 4888, 25, 3263, 268, 198, 4888, 1239, 268, 25, 220, 13655, 489, 8011, 198, 5953, 4396, 512, 12465, 535, 482, 16290, 1347, 5248, 800, 685, 25, 17382, 349, 25, 26572, 2740, 11, 362, 36436, 87, 8654, 16345, 813, 1828, 6913, 3440, 311, 8895, 220, 1135, 12306, 2134, 323, 3568, 21428, 36090, 660, 449, 1443, 18148, 79270, 320, 36, 8, 12306, 11204, 5674, 6273, 311, 29908, 7577, 449, 15936, 32932, 685, 320, 49, 8, 220, 19, 4, 25173, 220, 717, 4, 320, 31039, 389, 2237, 8, 315, 279, 2218, 596, 7340, 2890, 1174, 61246, 520, 220, 1041, 2403, 26872, 662, 362, 36436, 87, 88084, 369, 220, 1490, 4, 315, 279, 1772, 1474, 275, 18413, 12306, 5674, 27023, 11, 11293, 311, 220, 914, 4, 2403, 60905, 16853, 65361, 362, 36436, 87, 13280, 520, 3325, 832, 9354, 18824, 477, 3544, 18118, 449, 264, 6913, 3440, 389, 70201, 477, 459, 5845, 1174, 279, 44350, 315, 16290, 1347, 5248, 800, 685, 374, 11293, 555, 220, 17, 6622, 11, 11041, 311, 220, 19, 422, 568, 13280, 449, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 315, 578, 12538, 258, 37080, 16853, 48, 482, 578, 12538, 258, 37080, 25, 362, 36436, 87, 649, 20891, 578, 12538, 258, 37080, 2380, 3115, 1603, 279, 5845, 5900, 389, 44350, 11, 449, 264, 220, 16, 44963, 1118, 44350, 1990, 57133, 13, 1442, 362, 36436, 87, 1587, 539, 1421, 561, 279, 5845, 2949, 220, 19, 6622, 315, 279, 3766, 6445, 11, 433, 5900, 389, 44350, 627, 5847, 25, 362, 36436, 87, 27772, 264, 13471, 449, 813, 2294, 80138, 369, 1855, 315, 279, 2380, 57133, 11, 14892, 29908, 7577, 505, 4994, 5410, 21453, 2134, 7106, 5674, 311, 14207, 4295, 2949, 459, 3158, 13, 61581, 4295, 2949, 264, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 315, 279, 3158, 1935, 220, 1399, 4, 12306, 5674, 323, 1101, 33085, 709, 369, 220, 15, 13, 220, 914, 6622, 13, 9062, 17876, 6445, 12992, 578, 12538, 258, 37080, 596, 5674, 555, 220, 914, 126437, 5451, 11514, 25, 362, 36436, 87, 596, 1176, 13471, 22223, 264, 220, 15894, 25800, 220, 5245, 61675, 52524, 3158, 304, 279, 2218, 5216, 11, 449, 1461, 31288, 389, 279, 1203, 1584, 323, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 520, 279, 3117, 61943, 6964, 627, 16041, 11514, 25, 362, 36436, 87, 596, 2132, 13471, 22223, 264, 490, 2070, 89, 71916, 3158, 304, 279, 2218, 5216, 11, 449, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 520, 279, 3117, 61943, 6964, 13, 578, 4295, 2054, 12302, 220, 1041, 20486, 1220, 4920, 362, 36436, 87, 323, 2289, 220, 19799, 20486, 1220, 304, 4156, 315, 1461, 11, 30090, 1990, 220, 3101, 323, 220, 2636, 20486, 1220, 7029, 505, 4920, 311, 304, 4156, 627, 38075, 11514, 25, 362, 36436, 87, 596, 4948, 13471, 22223, 264, 220, 3101, 18180, 28029, 3158, 31288, 389, 264, 2218, 3813, 430, 374, 220, 1049, 8316, 304, 4156, 315, 1461, 11, 449, 264, 220, 5245, 18180, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 2949, 627, 791, 12538, 258, 37080, 12789, 220, 2131, 611, 220, 1399, 611, 220, 2397, 611, 220, 2031, 4, 320, 31039, 389, 2237, 8, 5674, 2403, 60905, 1174, 323, 279, 14459, 709, 8250, 505, 20129, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 374, 35717, 311, 220, 15, 13, 220, 20, 6622, 2403, 26872, 16853, 54, 482, 16290, 1347, 5248, 800, 685, 25, 17382, 349, 25, 26572, 2740, 11, 362, 36436, 87, 8654, 16345, 813, 1828, 6913, 3440, 311, 8895, 220, 1135, 12306, 2134, 323, 3568, 21428, 36090, 660, 449, 1443, 18148, 79270, 320, 36, 8, 12306, 11204, 5674, 6273, 311, 29908, 7577, 449, 15936, 32932, 685, 320, 49, 8, 220, 19, 4, 25173, 220, 717, 4, 320, 31039, 389, 2237, 8, 315, 279, 2218, 596, 7340, 2890, 1174, 61246, 520, 220, 1041, 2403, 26872, 662, 362, 36436, 87, 88084, 369, 220, 1490, 4, 315, 279, 1772, 1474, 275, 18413, 12306, 5674, 27023, 11, 11293, 311, 220, 914, 4, 2403, 60905, 16853, 65361, 362, 36436, 87, 13280, 520, 3325, 832, 9354, 18824, 477], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [128000, 4110, 264, 9130, 315, 42986, 18824, 2728, 279, 2768, 3649, 512, 17046, 25, 5473, 12393, 1232, 364, 13655, 518, 364, 12393, 5796, 8438, 1232, 14226, 8011, 518, 364, 12393, 1311, 4469, 1232, 364, 18, 518, 364, 12393, 1311, 4469, 5796, 8438, 1232, 14226, 15, 13, 20, 518, 364, 43648, 1232, 364, 1987, 518, 364, 43648, 5796, 8438, 1232, 14226, 19, 13, 23, 518, 364, 21208, 41995, 1232, 364, 1399, 518, 364, 21208, 41995, 5796, 8438, 1232, 14226, 20, 518, 364, 38551, 5023, 380, 1232, 364, 843, 518, 364, 38551, 5023, 380, 5796, 8438, 1232, 14226, 17, 13, 2304, 518, 364, 3479, 17374, 1232, 364, 12901, 518, 364, 21208, 9897, 1232, 364, 10005, 518, 364, 21208, 17374, 5796, 8438, 1232, 14226, 17, 13, 20, 39303, 364, 9416, 1232, 364, 3561, 268, 518, 364, 9416, 1311, 4469, 1232, 364, 13655, 489, 8011, 518, 364, 9416, 1311, 4469, 5796, 8438, 1232, 76452, 35003, 5144, 25, 16290, 1347, 5248, 800, 685, 11, 578, 12538, 258, 37080, 11, 16290, 1347, 5248, 800, 685, 11, 24218, 42743, 37770, 11, 4435, 469, 910, 198, 1999, 25, 510, 9514, 649, 923, 35641, 596, 538, 1618, 422, 499, 1390, 933, 32215, 279, 2539, 18824, 3649, 320, 2150, 11, 18000, 449, 28887, 11, 323, 264, 52322, 14646, 8, 304, 279, 1742, 315, 65806, 596, 4033, 18824, 6959, 627, 1163, 13580, 25, 362, 36436, 87, 198, 3936, 25, 279, 12538, 258, 37080, 198, 17046, 512, 14884, 25, 220, 13655, 198, 14884, 824, 2237, 25, 489, 8011, 198, 14884, 1239, 268, 25, 220, 18, 198, 14884, 1239, 268, 824, 2237, 25, 489, 15, 13, 20, 198, 63982, 25, 220, 1987, 198, 63982, 824, 2237, 25, 489, 19, 13, 23, 198, 29702, 5674, 25, 220, 1399, 198, 29702, 5674, 824, 2237, 25, 489, 20, 198, 44638, 22884, 25, 220, 843, 198, 44638, 22884, 824, 2237, 25, 489, 17, 13, 2304, 198, 10061, 4732, 25, 220, 12901, 198, 29702, 2134, 25, 220, 10005, 198, 29702, 4732, 824, 2237, 25, 489, 17, 13, 20, 14062, 4888, 25, 3263, 268, 198, 4888, 1239, 268, 25, 220, 13655, 489, 8011, 198, 5953, 4396, 512, 12465, 535, 482, 16290, 1347, 5248, 800, 685, 25, 17382, 349, 25, 26572, 2740, 11, 362, 36436, 87, 8654, 16345, 813, 1828, 6913, 3440, 311, 8895, 220, 1135, 12306, 2134, 323, 3568, 21428, 36090, 660, 449, 1443, 18148, 79270, 320, 36, 8, 12306, 11204, 5674, 6273, 311, 29908, 7577, 449, 15936, 32932, 685, 320, 49, 8, 220, 19, 4, 25173, 220, 717, 4, 320, 31039, 389, 2237, 8, 315, 279, 2218, 596, 7340, 2890, 1174, 61246, 520, 220, 1041, 2403, 26872, 662, 362, 36436, 87, 88084, 369, 220, 1490, 4, 315, 279, 1772, 1474, 275, 18413, 12306, 5674, 27023, 11, 11293, 311, 220, 914, 4, 2403, 60905, 16853, 65361, 362, 36436, 87, 13280, 520, 3325, 832, 9354, 18824, 477, 3544, 18118, 449, 264, 6913, 3440, 389, 70201, 477, 459, 5845, 1174, 279, 44350, 315, 16290, 1347, 5248, 800, 685, 374, 11293, 555, 220, 17, 6622, 11, 11041, 311, 220, 19, 422, 568, 13280, 449, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 315, 578, 12538, 258, 37080, 16853, 48, 482, 578, 12538, 258, 37080, 25, 362, 36436, 87, 649, 20891, 578, 12538, 258, 37080, 2380, 3115, 1603, 279, 5845, 5900, 389, 44350, 11, 449, 264, 220, 16, 44963, 1118, 44350, 1990, 57133, 13, 1442, 362, 36436, 87, 1587, 539, 1421, 561, 279, 5845, 2949, 220, 19, 6622, 315, 279, 3766, 6445, 11, 433, 5900, 389, 44350, 627, 5847, 25, 362, 36436, 87, 27772, 264, 13471, 449, 813, 2294, 80138, 369, 1855, 315, 279, 2380, 57133, 11, 14892, 29908, 7577, 505, 4994, 5410, 21453, 2134, 7106, 5674, 311, 14207, 4295, 2949, 459, 3158, 13, 61581, 4295, 2949, 264, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 315, 279, 3158, 1935, 220, 1399, 4, 12306, 5674, 323, 1101, 33085, 709, 369, 220, 15, 13, 220, 914, 6622, 13, 9062, 17876, 6445, 12992, 578, 12538, 258, 37080, 596, 5674, 555, 220, 914, 126437, 5451, 11514, 25, 362, 36436, 87, 596, 1176, 13471, 22223, 264, 220, 15894, 25800, 220, 5245, 61675, 52524, 3158, 304, 279, 2218, 5216, 11, 449, 1461, 31288, 389, 279, 1203, 1584, 323, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 520, 279, 3117, 61943, 6964, 627, 16041, 11514, 25, 362, 36436, 87, 596, 2132, 13471, 22223, 264, 490, 2070, 89, 71916, 3158, 304, 279, 2218, 5216, 11, 449, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 520, 279, 3117, 61943, 6964, 13, 578, 4295, 2054, 12302, 220, 1041, 20486, 1220, 4920, 362, 36436, 87, 323, 2289, 220, 19799, 20486, 1220, 304, 4156, 315, 1461, 11, 30090, 1990, 220, 3101, 323, 220, 2636, 20486, 1220, 7029, 505, 4920, 311, 304, 4156, 627, 38075, 11514, 25, 362, 36436, 87, 596, 4948, 13471, 22223, 264, 220, 3101, 18180, 28029, 3158, 31288, 389, 264, 2218, 3813, 430, 374, 220, 1049, 8316, 304, 4156, 315, 1461, 11, 449, 264, 220, 5245, 18180, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 2949, 627, 791, 12538, 258, 37080, 12789, 220, 2131, 611, 220, 1399, 611, 220, 2397, 611, 220, 2031, 4, 320, 31039, 389, 2237, 8, 5674, 2403, 60905, 1174, 323, 279, 14459, 709, 8250, 505, 20129, 279, 29908, 7577, 449, 19048, 5277, 36469, 320, 49, 8, 328, 26488, 19644, 374, 35717, 311, 220, 15, 13, 220, 20, 6622, 2403, 26872, 16853, 54, 482, 16290, 1347, 5248, 800, 685, 25, 17382, 349, 25, 26572, 2740, 11, 362, 36436, 87, 8654, 16345, 813, 1828, 6913, 3440, 311, 8895, 220, 1135, 12306, 2134, 323, 3568, 21428, 36090, 660, 449, 1443, 18148, 79270, 320, 36, 8, 12306, 11204, 5674, 6273, 311, 29908, 7577, 449, 15936, 32932, 685, 320, 49, 8, 220, 19, 4, 25173, 220, 717, 4, 320, 31039, 389, 2237, 8, 315, 279, 2218, 596, 7340, 2890, 1174, 61246, 520, 220, 1041, 2403, 26872, 662, 362, 36436, 87, 88084, 369, 220, 1490, 4, 315, 279, 1772, 1474, 275, 18413, 12306, 5674, 27023, 11, 11293, 311, 220, 914, 4, 2403, 60905, 16853, 65361, 362, 36436, 87, 13280, 520, 3325, 832, 9354, 18824, 477]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f86c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d50125d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device_map = {\"\": torch.cuda.current_device()} \n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1592ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # Adjust based on VRAM\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    fp16=True,  # If on Ampere/NVIDIA RTX (which 4060 is)\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c5cbf271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    \n",
    "    data_collator=data_collator,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a53c9d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 5:29:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.240900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.288500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.259400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.096800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.185400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.017000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.060100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.165500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.033200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.089900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.991800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=340, training_loss=1.2481605277341954, metrics={'train_runtime': 19841.2191, 'train_samples_per_second': 0.017, 'train_steps_per_second': 0.017, 'total_flos': 1.568460635111424e+16, 'train_loss': 1.2481605277341954, 'epoch': 1.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef3150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d4a3a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a91a2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.save_model(\"./results/checkpoint-final\")\n",
    "tokenizer.save_pretrained(\"./results/checkpoint-final\")\n",
    "trainer.save_state()  # if you want to resume optimizer/scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057a5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
